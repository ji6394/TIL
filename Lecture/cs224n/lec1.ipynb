{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture1 : Introduction and Word Vectors\n",
    "1. The course\n",
    "2. Human language and word meaning\n",
    "- complex\n",
    "- 각자 다르게 해석함\n",
    "- 언어는 사람이 구성하고 해석하는 사회적 시스템\n",
    "- 언어는 등장한지 그래 오래 되지 않음\n",
    "- 컴퓨터가 어떻게 인간의 언어를 이해하게 할지를 설계하는 것\n",
    "- Machine translation은 굉장히 발달함\n",
    "- GPT-3는 OPEN AI에서 만든 굉장한 거대 언어 모델. 이전의 단어를 기반으로 다음 단어를 예측함.\n",
    "- (~16:20)\n",
    "- Wordnet(like 사전 or 낱말집)같은 resource들의 문제점\n",
    "- 뉘앙스를 이해하지 못함\n",
    "- 단어의 신의미를 이해하지 못함\n",
    "- 주관적임\n",
    "- 비슷한 단어 간의 차이를 이해하지 못함\n",
    "- 과거의 NLP에서는 단어가 discrete symbols로 인식됨. 원핫벡터를 사용함. Vocabulary의 사이즈가 너무 커짐.\n",
    "- 단어 간의 similarity와 관계를 파악하지 못함\n",
    "- 단어들의 관계가 orthogonal\n",
    "- 이를 해결하기 위해 WordNet의 동의어, 유의어 리스트를 활용하여 similarity를 구할 수 있지 않겠느냐 하는 아이디어가 있었지만, 인간이 일일이 구축해야 하는만큼 incompleteness의 문제점이 있었음.\n",
    "- 이를 해결하기 위해 각각의 단어들에 similarity로 대변되는 실제 vector를 인코딩함.\n",
    "- 그 첫번째가 '분포가설'(distributional semantics) : 단어의 의미는 그 단어의 근처에 자주 나타나는 다른 단어들에 의해 부여될 것이라는 아이디어. context words라고 하죠. \n",
    "- nlp에서 단어들은 types와 tokens로 분류될 수 있음 \n",
    "- distributional semantics를 통해 각 단어의 dense한 real vector를 얻어냄. 단어들을 벡터화하게 되면 굉장히 고차원의 벡터가 될 수 밖에 없는데 이를 dense한 vector로 만드는 과정을 word embedding이라고 함.  비슷한 단어끼리 embedding이 모여있게 됨. \n",
    "3. Word2vec Introduction : 단어들을 embedding하는 알고리즘 중 하나\n",
    "- corpus(많은 양의 단어들) 내의 각 단어를 벡터로 표현. distributional similarity task를 통해 다른 단어들의 맥락에서 어떤 단어가 등장할 것인지 예측하는 것이 주요 목표. 현재 갖고 있는 word vector를 기반으로 target word(C)가 주어졌을 때, context words(O)가 등장할 확률이 어느 정도인지를 계산하고 실제 어떤 단어들이 등장하는지를 알기에 이를 토대로 실제 등장하는 단어들의 확률을 최대화하는 작업임. 다시 말해, word vector의 similarity를 이용하여 해당 단어가 등장할 확률을 계산하고 확률을 최대화하기 위해 word vector를 계속해서 변경하는 과정. 역으로 context word에 따라 center word를 추적하는 과정 역시 가능\n",
    "- Corpus에서 각 단어의 위치인 t마다 target word인 t가 주어졌을 때, window size인 m만큼의 범위 내에 context words를 예측하는 과정!\n",
    "- Data Likelihood = L(theta) = 각 자리별로 target word를 옮겨가며 target word가 등장했을 때 window size내 context word가 등장할 확률을 모두 곱함.\n",
    "학습하기 위해  Objective Function(cost function, loss function)이 필요함. objective function은 극대화하는 것이 아니라 극소화해야 전체 정확도가 높아지기 때문에 앞에 -가 붙음. 또한 곱보다 합을 다루는 것이 편하기에 log를 붙히며 average log likelihood를 사용하기 위해 총 단어의 수인 T만큼 나누어줌. 그렇다면 이 모든 것을 하기 위해 center word에 대한 context word의 등장 확률을 어떻게 구할까??\n",
    "4. Word2vec objectvie function gradietns\n",
    "- 각 단어마다 두 가지의 벡터를 부여한다! 하나는 해당 단어가 ceneter word 일 때의 word vector, 나머지 하나는 context word에 해당했을 때 word vector.\n",
    "이후, center word에 대한 context word의 등장 확률 = exp(해당 center word 벡터와 context 벡터의 내적)/전체 word에 대해 exp(각 단어의 context vector와 해당 center word벡터의 내적)으로 계산함. word vector간의 dot product는 두 단어 간의 유사성을 보여줌. exponentioal을 취하는 것은 내적 후 음수가 되었을 때를 대비하여 결과값을 양수로 만들기 위해 사용.\n",
    " softmax function을 활용하여 center word가 주어질 때, context word가 나올 조건부 확률을 계산함. 왜냐하면 softmax 함수는 어떤 수가 들어와도 0과 1사이의 일종의 확률값으로 바꾸어주기 때문이다! 이 때, context word와 center word를 내적하는 이유는 두 단어 간의 similarity를 계산해주기 때문. 내적이 클수록 유사성이 크다고 볼 수 있다. 또한 여기에 지수함수를 취하게 되는데 이는 벡터 간 내적이 음수로 나올 수 있는데 모두 양수를 보장하기 위하여다. 이를 center word와 vocabulary 내 모든 단어들의 내적을 지수함수 취한 것의 합으로 나누어주는 softmax 함수는 값을 0과 1사이로 도출하기 때문에 확률값으로 이용할 수 있다. soft는 작은 x값에도 어떤 확률 값을 부여하기 때문에 붙여졌고 max는 지수함수의 특성으로 큰 x의 확률값을 amplify(상세히 하다)하기 때문에 붙여짐.\n",
    "5. Optimization basics\n",
    "theta는 모델 파라미터를 의미함. word2vec에서는 word vector가 파라미터에 해당. 두개의 벡터 존재. d차원의 벡터를 갖는 V개의 vocabulary. 따라서 2dv의 긴 벡터가 생성됨. 벡터 그라디언트를 계산하여 미분하는 작업이 필요함.\n",
    "(~56:52) : Chain rule 수식 전개 보여줌\n",
    "6. Looking at word vectors\n",
    "![1](C:/Users/82108/Desktop/정지훈/til/TIL/image/1.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.decomposition import PCA\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVect ors\n",
    "\n",
    "model = api.load(\"glove-wiki-gigaword-100\")\n",
    "print(type(model))\n",
    "\n",
    "model['bread']\n",
    "model['croissant']\n",
    "\n",
    "model.most_similar('usa')\n",
    "model.most_similar('banana')\n",
    "\n",
    "result = model.most_similar(positive=['woman','king'], negative=['man'])\n",
    "print('{}:{:.4f}'.format(*result[0]))\n",
    "\n",
    "def analogy(x1, x2, y1):\n",
    "  result = model.most_similar(positive=[y1,x2], negative = [x1])\n",
    "  return result[0][0]\n",
    "analogy('man','king','woman')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
