{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture1 : Introduction and Word Vectors\n",
    "1. The course\n",
    "2. Human language and word meaning\n",
    "- complex\n",
    "- 각자 다르게 해석함\n",
    "- 언어는 사람이 만든 체계\n",
    "- 언어는 등장한지 그래 오래 되지 않음\n",
    "- 컴퓨터가 어떻게 사람의 말을 이해하게 할지를 설계하는 것\n",
    "- Machine translation은 굉장히 발달함\n",
    "- GPT-3는 OPEN AI에서 만든 굉장한 거대 언어 모델. 이전의 단어를 기반으로 다음 단어를 예측함.\n",
    "- (~16:20)\n",
    "- Wordnet같은 resource들의 문제점\n",
    "- - 뉘앙스를 이해하지 못함\n",
    "- - 단어의 신의미를 이해하지 못함\n",
    "- - 비슷한 단어 간의 차이를 이해하지 못함\n",
    "- 과거의 NLP에서는 단어가 discrete symbols로 인식됨. 원핫벡터를 사용함. Vocabulary의 사이즈가 너무 커짐.\n",
    "- 단어 간의 similarity와 관계를 파악하지 못함\n",
    "- orthogonal : 직교의\n",
    "- 이를 해결하기 위해 현대의 nlp 기법들 등장\n",
    "- 그 첫번째가 distributional semantics : 단어의 의미는 그 단어에 빈번하게 근접하게 나타나는 다른 단어들에 의해 부여됨. context words라고 하죠. \n",
    "- nlp에서 단어들은 types와 tokens로 분류될 수 있음\n",
    "- distributional semantics를 통해 각 단어의 dense한 real vector를 얻어냄. 비슷한 단어끼리 embedding이 모여있게 됨. \n",
    "3. Word2vec Introduction\n",
    "- corpus 내의 각 단어를 벡터로 표현. 모든 위치마다 target word와 context words간의 word vector의 similarity를 이용하여 해당 단어가 등장할 확률을 계산하고 확률을 최대화하기 위해 word vector를 계속해서 변경하는 과정. \n",
    "- Corpus에서 각 단어의 위치인 t마다 target word로 설정. 이에 따라 context words를 예측. window size는 m으로 설정. Data Likelihood = L(theta) = 각 자리별로 target word를 옮겨가며 target word가 등장했을 때 window size내 context word의 확률을 모두 곱함.\n",
    "이에 따른 Objective Function(cost function, loss function)은 극대화하는 것이 아니라 극소화해야 전체 정확도가 높아지기 때문에 앞에 -가 붙음. 또한 곱보다 합을 다루는 것이 편하기에 log를 붙힘.\n",
    "4. Word2vec objectvie function gradietns\n",
    "- 각 단어마다 target word에 해당했을 때 word vector와 context word에 해당했을 때 word vector의 두가지 벡터 부여. softmax function을 활용하여 center word가 주어질 때, context word가 나올 조건부 확률을 계산함. 이 때, context word와 center word를 내적하는 이유는 두 단어 간의 similarity를 계산해주기 때문. 내적이 클수록 유사성이 크다고 볼 수 있다. 또한 여기에 지수함수를 취하게 되는데 이는 벡터 간 내적이 음수로 나올 수 있는데 모두 양수를 보장하기 위하여다. 이를 center word와 vocabulary 내 모든 단어들의 내적을 지수함수 취한 것의 합으로 나누어주는 softmax 함수는 값을 0과 1사이로 도출하기 때문에 확률값으로 이용할 수 있다. soft는 작은 x값에도 어떤 확률 값을 부여하기 때문에 붙여졌고 max는 지수함수의 특성으로 큰 x의 확률값을 amplify(상세히 하다)하기 때문에 붙여짐.\n",
    "5. Optimization basics\n",
    "theta는 모델 파라미터를 의미함. word2vec에서는 word vector가 파라미터에 해당. 두개의 벡터 존재. d차원, V개의 vocabulary. 따라서 2dv의 긴 벡터가 생성됨. 벡터 그라디언트를 계산하여 미분하는 작업이 필요함.\n",
    "(~56:52) : Chain rule 수식 전개 보여줌\n",
    "6. Looking at word vectors\n",
    "![1](C:/Users/82108/Desktop/정지훈/til/TIL/image/1.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.decomposition import PCA\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = api.load(\"glove-wiki-gigaword-100\")\n",
    "print(type(model))\n",
    "\n",
    "model['bread']\n",
    "model['croissant']\n",
    "\n",
    "model.most_similar('usa')\n",
    "model.most_similar('banana')\n",
    "\n",
    "result = model.most_similar(positive=['woman','king'], negative=['man'])\n",
    "print('{}:{:.4f}'.format(*result[0]))\n",
    "\n",
    "def analogy(x1, x2, y1):\n",
    "  result = model.most_similar(positive=[y1,x2], negative = [x1])\n",
    "  return result[0][0]\n",
    "\n",
    "analogy('man','king','woman')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
