{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동화하기\n",
    "# 이어서 쓸 때 save_to_file과 append_to_file 함수에 mode의 파라미터를 'a'로 변경해보기\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from fake_useragent import UserAgent\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling failed, retrying... (1/5)\n",
      "Crawling failed, retrying... (2/5)\n",
      "Crawling failed, retrying... (3/5)\n",
      "Crawling failed, retrying... (4/5)\n",
      "Crawling failed, retrying... (5/5)\n",
      "Crawling failed after maximum retries.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_info(url, wait_time = 3, delay_time = 1):\n",
    "    data={}\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chrome_options = Options()\n",
    "            chrome_options.add_argument('--headless')  # Headless 모드 활성화\n",
    "            driver = webdriver.Chrome(options=chrome_options)\n",
    "            driver.implicitly_wait(wait_time)\n",
    "            driver.get(url)\n",
    "            html = driver.page_source\n",
    "            time.sleep(delay_time)\n",
    "            driver.quit()\n",
    "            \n",
    "            soup = BeautifulSoup(html,'html.parser')\n",
    "           \n",
    "            title = soup.find('h3', {'class': 'tit_subject'}).get_text(strip=True)\n",
    "            user_id = soup.find('span', {'class':'txt_subject'}).split('|')[0].replace('작성자','').strip()\n",
    "            post_time = soup.find('span', {'class':'txt_subject'}).split('|')[1].replace('작성시간','').strip()\n",
    "            post_time = datetime.strptime(post_time, '%Y-%m-%d')\n",
    "            post = soup.find(id='article').get_text(strip=True)\n",
    "            view_cnt = soup.find('span', {'class':'txt_subject'}).split('|')[2].replace('조회수','').strip()\n",
    "            reply_cnt = int(soup.find(class_=\"num_total\").get_text(strip=True))\n",
    "            reply_content = []\n",
    "            if reply_cnt != 0:\n",
    "                comment_button = driver.find_element(By.CLASS_NAME, 'link_all')\n",
    "                comment_button.click()\n",
    "                \n",
    "                # 페이지 내의 코멘트들을 모두 찾자\n",
    "                cmt_all = soup.body.find('ul', id = 'commentList').find_all('li')\n",
    "    \n",
    "                # 찾은 코멘트들을 언패킹하자\n",
    "                for k in cmt_all:\n",
    "                    if k.div.span.find_all('span', class_ = 'txt_bar'):\n",
    "                        if k['class'] == ['reply_on']:\n",
    "                            cmt_reply = True\n",
    "                        else:\n",
    "                            cmt_reply = False\n",
    "                            \n",
    "                        cmt_writer = k.div.find('span', class_='sr_only').next_sibling\n",
    "                        cmt_time = k.div.span.find('span', class_='num_info').get_text()\n",
    "                        cmt_txt = k.div.find('span', class_='txt_detail').get_text(strip=True)\n",
    "                        reply_content.append({'author': cmt_writer, 'text': cmt_txt, 'time' : cmt_time})\n",
    "            data = {'title': title, 'user_id': user_id, 'post_time': post_time, 'post': post, 'view_cnt': view_cnt, 'reply_cnt': reply_cnt, 'reply_content': reply_content}\n",
    "            \n",
    "            # 댓글이 있다면 딕셔너리에 추가\n",
    "            if reply_cnt != 0:\n",
    "                for i, reply_info in enumerate(reply_content):\n",
    "                    data[f'reply_{i+1}_author'] = reply_info['author']\n",
    "                    data[f'reply_{i+1}_text'] = reply_info['text']\n",
    "                    data[f'reply_{i+1}_time'] = reply_info['time']\n",
    "            print(url, '완료')\n",
    "            break  # 성공적으로 크롤링이 완료되면 루프 종료\n",
    "        except Exception as e:\n",
    "            print(f\"Crawling failed, retrying... ({attempt+1}/{max_retries})\")\n",
    "            time.sleep(2)  # 2초 대기 후 재시도\n",
    "    else:\n",
    "        print(\"Crawling failed after maximum retries.\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "extract_info('https://m.cafe.daum.net/rocksoccer/ADs2/488585')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "# 설치 폴더에 주의합니다. \n",
    "\n",
    "driver.get('https://logins.daum.net/accounts/loginform.do?mobilefull=1&category=cafe&url=http%3A%2F%2Fm.cafe.daum.net%2F_myCafe%3Fnull')\n",
    "# 19년 5월부터 로그인 페이지 주소가 살짝 바뀌었네요. \n",
    "\n",
    "time.sleep(3)\n",
    "# 페이지 전환시에는 적당한 시간을 줍니다. \n",
    "# 1. 과도한 크롤링 방지.\n",
    "# 2. 페이지 전환이 완료되기 전에 다음 명령 실행되는 것 방지.\n",
    "#    AJAX를 사용한 페이지는 페이지 전환시 딜레이가 꼭 필요한 경우도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(url, wait_time=3, delay_time=1):\n",
    "    max_retries = 2\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chrome_options = Options()\n",
    "            # chrome_options.add_argument('--headless')  # Headless 모드 활성화\n",
    "            # 원하는 User-Agent 설정\n",
    "            driver.get(url)\n",
    "            html = driver.page_source\n",
    "            time.sleep(delay_time)\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            title = soup.find('h3', class_= 'tit_subject').get_text(strip=True)\n",
    "            user_info = soup.find('span', {'class': 'txt_subject'}).get_text(\"|\", strip=True).split('|')\n",
    "            user_id = user_info[0].replace('작성자', '').strip()\n",
    "            post_time = user_info[1].replace('작성시간', '').strip()\n",
    "            post_time = datetime.strptime(post_time, '%Y-%m-%d')\n",
    "            post = soup.find(id='article').get_text(strip=True)\n",
    "            view_cnt = user_info[2].replace('조회수', '').strip()\n",
    "            reply_cnt = int(soup.find(class_=\"num_total\").get_text(strip=True))\n",
    "            reply_content = []\n",
    "\n",
    "            print(f\"title: {title}\")\n",
    "            print(f\"user_id: {user_id}\")\n",
    "            print(f\"post_time: {post_time}\")\n",
    "            print(f\"post: {post}\")\n",
    "            print(f\"view_cnt: {view_cnt}\")\n",
    "            print(f\"reply_cnt: {reply_cnt}\")\n",
    "\n",
    "            if reply_cnt != 0:\n",
    "                comment_button = driver.find_element(By.CLASS_NAME, 'link_all')\n",
    "                comment_button.click()\n",
    "                time.sleep(1)  # 동적 콘텐츠 로딩 대기\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                cmt_all = soup.body.find('ul', id='commentList').find_all('li')\n",
    "\n",
    "                for k in cmt_all:\n",
    "                    if k.div.span.find_all('span', class_='txt_bar'):\n",
    "                        if 'reply_on' in k['class']:\n",
    "                            cmt_reply = True\n",
    "                        else:\n",
    "                            cmt_reply = False\n",
    "\n",
    "                        cmt_writer = k.div.find('span', class_='sr_only').next_sibling\n",
    "                        cmt_time = k.div.span.find('span', class_='num_info').get_text()\n",
    "                        cmt_txt = k.div.find('span', class_='txt_detail').get_text(strip=True)\n",
    "                        reply_content.append({'author': cmt_writer, 'text': cmt_txt, 'time': cmt_time})\n",
    "                        print(f\"comment: {cmt_writer}, {cmt_txt}, {cmt_time}\")\n",
    "\n",
    "            driver.quit()\n",
    "\n",
    "            print(url, '완료')\n",
    "            break  # 성공적으로 크롤링이 완료되면 루프 종료\n",
    "        except Exception as e:\n",
    "            print(f\"Crawling failed, retrying... ({attempt + 1}/{max_retries})\")\n",
    "            \n",
    "            time.sleep(2)  # 2초 대기 후 재시도\n",
    "    else:\n",
    "        print(\"Crawling failed after maximum retries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling failed, retrying... (1/2)\n",
      "Crawling failed, retrying... (2/2)\n",
      "Crawling failed after maximum retries.\n"
     ]
    }
   ],
   "source": [
    "extract_info('https://m.cafe.daum.net/rocksoccer/ADs2/488585')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sda",
   "language": "python",
   "name": "sda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
