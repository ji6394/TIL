{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동화하기\n",
    "# 이어서 쓸 때 save_to_file과 append_to_file 함수에 mode의 파라미터를 'a'로 변경해보기\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from fake_useragent import UserAgent\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\n",
      "Reached the last page.\n",
      "총 32개의 페이지가 있습니다\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "총 636개의 글 링크를 찾았습니다\n",
      "Crawling failed, retrying... (1/5)\n",
      "Crawling failed, retrying... (2/5)\n",
      "Crawling failed, retrying... (3/5)\n",
      "Crawling failed, retrying... (4/5)\n",
      "Crawling failed, retrying... (5/5)\n",
      "Crawling failed after maximum retries.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 198\u001b[0m\n\u001b[0;32m    196\u001b[0m rocksoccer_results \u001b[38;5;241m=\u001b[39m save_to_file()\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# 게시글 정보 크롤링 및 저장\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m \u001b[43mget_contents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 150\u001b[0m, in \u001b[0;36mget_contents\u001b[1;34m()\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m post_link \u001b[38;5;129;01min\u001b[39;00m post_links:\n\u001b[0;32m    149\u001b[0m     content \u001b[38;5;241m=\u001b[39m extract_info(post_link)\n\u001b[1;32m--> 150\u001b[0m     \u001b[43mappend_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRockSoccer_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mQUERY\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPAGES\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m모든 작업이 완료되었습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 166\u001b[0m, in \u001b[0;36mappend_to_file\u001b[1;34m(file_name, data)\u001b[0m\n\u001b[0;32m    162\u001b[0m writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(file)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Write main post data to CSV\u001b[39;00m\n\u001b[0;32m    165\u001b[0m main_post_row \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 166\u001b[0m     \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m    167\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    168\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost_time\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    169\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    170\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mview_cnt\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    171\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreply_cnt\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    172\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreply_author\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    173\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreply_text\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    174\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreply_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    175\u001b[0m ]\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# Check if there are replies\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreply_content\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreply_cnt\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# Write main post data along with reply data to CSV\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'title'"
     ]
    }
   ],
   "source": [
    "options = Options()\n",
    "ua = UserAgent()\n",
    "userAgent = ua.random\n",
    "print(userAgent)\n",
    "options.add_argument(f'user-agent={userAgent}')\n",
    "driver = webdriver.Chrome()\n",
    "# 설치 폴더에 주의합니다. \n",
    "\n",
    "driver.get('https://logins.daum.net/accounts/loginform.do?mobilefull=1&category=cafe&url=http%3A%2F%2Fm.cafe.daum.net%2F_myCafe%3Fnull')\n",
    "# 19년 5월부터 로그인 페이지 주소가 살짝 바뀌었네요. \n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "#쿠팡 검색\n",
    "QUERY = '쿠팡'\n",
    "search_QUERY = urlencode({'subquery': QUERY}, encoding='utf-8')\n",
    "URL = 'https://m.cafe.daum.net/rocksoccer/ADs2/search/all?query={search_QUERY}&head=&r=https%3A%2F%2Fm.cafe.daum.net%2Frocksoccer%2FADs2'\n",
    "\n",
    "# 마지막 페이지까지 클릭\n",
    "def go_to_last_page(url):\n",
    "    driver.get(url)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    while True:\n",
    "        try:\n",
    "            # '다음 페이지' 버튼을 찾기\n",
    "            next_button = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a.btn_page.btn_next\")))\n",
    "            next_button_text = next_button.text.strip()\n",
    "            # 버튼이 비활성화 상태인지 확인\n",
    "            if next_button_text == '다음 목록이 없습니다.':\n",
    "                print(\"Reached the last page.\")\n",
    "                break\n",
    "\n",
    "            # '다음 페이지' 버튼 클릭\n",
    "            next_button.click()\n",
    "            # 로딩 시간을 기다리기 위해 sleep 사용\n",
    "            time.sleep(5)\n",
    "        except TimeoutException:\n",
    "            print(\"Reached the last page.\")\n",
    "            break\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    driver.quit()\n",
    "    return soup\n",
    "\n",
    "# 마지막 페이지 번호 알아내기\n",
    "def get_last_page(url):\n",
    "    soup = go_to_last_page(url)\n",
    "    page_list = []\n",
    "    # 모든 'num_page' 클래스를 가진 span 태그 내에서 페이지 번호 추출\n",
    "    for span in soup.find_all(\"span\", class_=\"num_page\"):\n",
    "        # 'link_page' 클래스를 가진 태그(a 태그 또는 em 태그) 찾기\n",
    "        page_tag = span.find(class_=\"link_page\")\n",
    "        if page_tag:\n",
    "            # 페이지 번호를 리스트에 추가\n",
    "            page_list.append(page_tag.get_text(strip=True))\n",
    "    max_page = page_list[-1]\n",
    "    print(f\"총 {max_page}개의 페이지가 있습니다\")\n",
    "    return max_page\n",
    "\n",
    "# 페이지 링크 모두 가져오기\n",
    "def get_boards(page_num):\n",
    "    boards=[]\n",
    "    for page in range(page_num):\n",
    "        boards.append(f\"https://m.cafe.daum.net/rocksoccer/ADs2/search/all?query=%EC%BF%A0%ED%8C%A1&r=https%3A%2F%2Fm.cafe.daum.net%2Frocksoccer%2FADs2&sort=0&head=&page={page+1}\")\n",
    "    return boards\n",
    "\n",
    "# 게시글 링크 가져오기\n",
    "def get_posts():\n",
    "    global QUERY\n",
    "    global PAGES\n",
    "    board_links = get_boards(PAGES)\n",
    "    posts = []\n",
    "    for board_link in board_links:\n",
    "        req = requests.get(board_link)\n",
    "        print(req.status_code)\n",
    "        soup = BeautifulSoup(req.text, 'html.parser')\n",
    "        tds = soup.find_all('a', {'class':'link_cafe #search_result_list'})\n",
    "        for td in tds:\n",
    "            if td is not None:\n",
    "               posts.append(td['href'])\n",
    "    print(f\"총 {len(posts)}개의 글 링크를 찾았습니다\")\n",
    "    \n",
    "    # 게시글 링크 csv로 저장\n",
    "    post_file = open(f\"ROCKSOCCER_{QUERY}_{PAGES}pages_inner_links.csv\", mode='w', encoding='utf-8')\n",
    "    writer = csv.writer(post_file)\n",
    "    for post in posts:\n",
    "        writer.writerow([post])\n",
    "    post_file.close()\n",
    "    return posts\n",
    "\n",
    "def extract_info(url, wait_time = 3, delay_time = 1):\n",
    "    max_retries = 5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            html = driver.page_source\n",
    "            time.sleep(delay_time)\n",
    "            soup = BeautifulSoup(html,'html.parser')\n",
    "           \n",
    "            title = soup.find('h3', {'class': 'tit_subject'}).get_text(strip=True)\n",
    "            user_id = soup.find('span', {'class':'txt_subject'}).split('|')[0].replace('작성자','').strip()\n",
    "            post_time = soup.find('span', {'class':'txt_subject'}).split('|')[1].replace('작성시간','').strip()\n",
    "            post_time = datetime.strptime(post_time, '%Y-%m-%d')\n",
    "            post = soup.find(id='article').get_text(strip=True)\n",
    "            view_cnt = soup.find('span', {'class':'txt_subject'}).split('|')[2].replace('조회수','').strip()\n",
    "            reply_cnt = int(soup.find(class_=\"num_total\").get_text(strip=True))\n",
    "            reply_content = []\n",
    "            if reply_cnt != 0:\n",
    "                comment_button = driver.find_element(By.CLASS_NAME, 'link_all')\n",
    "                comment_button.click()\n",
    "                \n",
    "                # 페이지 내의 코멘트들을 모두 찾자\n",
    "                cmt_all = soup.body.find('ul', id = 'commentList').find_all('li')\n",
    "    \n",
    "                # 찾은 코멘트들을 언패킹하자\n",
    "                for k in cmt_all:\n",
    "                    if k.div.span.find_all('span', class_ = 'txt_bar'):\n",
    "                        if k['class'] == ['reply_on']:\n",
    "                            cmt_reply = True\n",
    "                        else:\n",
    "                            cmt_reply = False\n",
    "                            \n",
    "                        cmt_writer = k.div.find('span', class_='sr_only').next_sibling\n",
    "                        cmt_time = k.div.span.find('span', class_='num_info').get_text()\n",
    "                        cmt_txt = k.div.find('span', class_='txt_detail').get_text(strip=True)\n",
    "                        reply_content.append({'author': cmt_writer, 'text': cmt_txt, 'time' : cmt_time})\n",
    "            data = {'title': title, 'user_id': user_id, 'post_time': post_time, 'post': post, 'view_cnt': view_cnt, 'reply_cnt': reply_cnt, 'reply_content': reply_content}\n",
    "            \n",
    "            # 댓글이 있다면 딕셔너리에 추가\n",
    "            if reply_cnt != 0:\n",
    "                for i, reply_info in enumerate(reply_content):\n",
    "                    data[f'reply_{i+1}_author'] = reply_info['author']\n",
    "                    data[f'reply_{i+1}_text'] = reply_info['text']\n",
    "                    data[f'reply_{i+1}_time'] = reply_info['time']\n",
    "            print(url, '완료')\n",
    "            break  # 성공적으로 크롤링이 완료되면 루프 종료\n",
    "        except Exception as e:\n",
    "            print(f\"Crawling failed, retrying... ({attempt+1}/{max_retries})\")\n",
    "            time.sleep(2)  # 2초 대기 후 재시도\n",
    "    else:\n",
    "        print(\"Crawling failed after maximum retries.\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_contents():\n",
    "    global rocksoccer_results\n",
    "    post_links = get_posts()\n",
    "    for post_link in post_links:\n",
    "        content = extract_info(post_link)\n",
    "        append_to_file(f\"RockSoccer_{QUERY}_{PAGES}.csv\", content)\n",
    "    return print(\"모든 작업이 완료되었습니다.\")\n",
    "\n",
    "def save_to_file():\n",
    "    file = open(f'RockSoccer_{QUERY}_.csv', mode='a', encoding='utf-8')\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['title', 'user_id', 'post_time', 'post', 'view_cnt', 'reply_cnt', 'reply_author', 'reply_text', 'reply_time'])\n",
    "    file.close()\n",
    "    return file\n",
    "\n",
    "def append_to_file(file_name, data):\n",
    "    file = open(file_name, mode='a', encoding='utf-8', newline='')  # newline 추가\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write main post data to CSV\n",
    "    main_post_row = [\n",
    "        data['title'],\n",
    "        data['user_id'],\n",
    "        data['post_time'],\n",
    "        data['post'],\n",
    "        data['view_cnt'],\n",
    "        data['reply_cnt'],\n",
    "        data['reply_author'],\n",
    "        data['reply_text'],\n",
    "        data['reply_time']\n",
    "    ]\n",
    "\n",
    "    # Check if there are replies\n",
    "    if 'reply_content' in data and data['reply_cnt'] != 0:\n",
    "        # Write main post data along with reply data to CSV\n",
    "        for i, reply_info in enumerate(data['reply_content']):\n",
    "            writer.writerow(main_post_row + [\n",
    "                reply_info.get('author', ''),  # Reply author\n",
    "                reply_info.get('text', ''),  # Reply text\n",
    "                reply_info.get('time','')\n",
    "            ])\n",
    "    else:\n",
    "        # Write only main post data to CSV\n",
    "        writer.writerow(main_post_row + ['']*3)\n",
    "\n",
    "    file.close()\n",
    "    return\n",
    "\n",
    "# 크롤링할 페이지 수 설정\n",
    "PAGES = int(get_last_page(URL))\n",
    "# 게시글 링크 수집\n",
    "rocksoccer_results = save_to_file()\n",
    "# 게시글 정보 크롤링 및 저장\n",
    "get_contents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sda",
   "language": "python",
   "name": "sda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
