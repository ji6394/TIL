{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Using cached selenium-4.21.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting urllib3<3,>=1.26 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Using cached trio-0.25.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Using cached trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\82108\\anaconda3\\envs\\bearstxt\\lib\\site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\82108\\anaconda3\\envs\\bearstxt\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\82108\\anaconda3\\envs\\bearstxt\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\82108\\anaconda3\\envs\\bearstxt\\lib\\site-packages (from trio~=0.17->selenium) (2.6)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\82108\\anaconda3\\envs\\bearstxt\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\82108\\anaconda3\\envs\\bearstxt\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\82108\\anaconda3\\envs\\bearstxt\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\82108\\anaconda3\\envs\\bearstxt\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\82108\\anaconda3\\envs\\bearstxt\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Using cached selenium-4.21.0-py3-none-any.whl (9.5 MB)\n",
      "Using cached trio-0.25.1-py3-none-any.whl (467 kB)\n",
      "Using cached trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, urllib3, pysocks, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.22\n",
      "    Uninstalling urllib3-1.22:\n",
      "      Successfully uninstalled urllib3-1.22\n",
      "Successfully installed outcome-1.3.0.post0 pysocks-1.7.1 selenium-4.21.0 sortedcontainers-2.4.0 trio-0.25.1 trio-websocket-0.11.1 urllib3-2.2.1 wsproto-1.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "botocore 1.18.18 requires urllib3<1.26,>=1.20; python_version != \"3.4\", but you have urllib3 2.2.1 which is incompatible.\n",
      "requests 2.18.4 requires urllib3<1.23,>=1.21.1, but you have urllib3 2.2.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m urlencode\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Options\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfake_useragent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserAgent\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "# 자동화하기\n",
    "# 이어서 쓸 때 save_to_file과 append_to_file 함수에 mode의 파라미터를 'a'로 변경해보기\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from fake_useragent import UserAgent\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 설정\n",
    "QUERY = '쿠팡'\n",
    "search_QUERY = urlencode({'subquery': QUERY}, encoding='utf-8')\n",
    "URL = f\"https://www.fmkorea.com/search.php?mid=football_korean&search_keyword={QUERY}&search_target=title_content\"\n",
    "\n",
    "def get_posts(page_num):\n",
    "    global QUERY\n",
    "    posts = []\n",
    "    for page in range(page_num + 1):\n",
    "        # 페이지 넘기기\n",
    "        board_link = f\"https://www.fmkorea.com/search.php?mid=football_korean&search_keyword={QUERY}&search_target=title_content&page={page+1}\"\n",
    "        req = requests.get(board_link)\n",
    "        print(req.status_code)\n",
    "        soup = BeautifulSoup(req.text, 'html.parser')\n",
    "        # 전체 20개의 게시글의 링크를 posts에 append하는 과정\n",
    "        tds = soup.find_all('td', {'class': 'title hotdeal_var8'})\n",
    "        for td in tds:\n",
    "            post = td.find('a', {'class': 'hx'})\n",
    "            if post is not None:\n",
    "                posts.append(post['href'])\n",
    "    print(f\"총 {len(posts)}개의 글 링크를 찾았습니다.\")\n",
    "    # 게시글 링크 csv로 저장\n",
    "    post_file = open(f\"FMkorea_{QUERY}_{page_num}pages_inner_links.csv\", mode='w', encoding='utf-8')\n",
    "    writer = csv.writer(post_file)\n",
    "    for post in posts:\n",
    "        writer.writerow([post])\n",
    "    post_file.close()\n",
    "    return posts\n",
    "\n",
    "def extract_info(url):\n",
    "    chrome_options = Options()\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    title = soup.find('span', {'class': 'np_18px_span'}).get_text(strip=True)\n",
    "    user_id = soup.find_all('a', {'class': lambda x: x and x.startswith('member_')}).get_text(strip=True)\n",
    "    post_time = soup.find('span', {'class': 'date m_no'}).get_text(strip=True)\n",
    "    post = soup.find('div', class_='xe_content').get_text(seperator = ' ', strip=True)\n",
    "    view_cnt = int(soup.find('span', text=lambda x: x and '조회 수' in x).find('b').text)\n",
    "    recomm_cnt = int(soup.find('span', text=lambda x: x and '추천 수' in x).find('b').text)\n",
    "    reply_cnt = int(soup.find('b').text)\n",
    "    reply_content = []#여기서부터 수정!! 댓글수가 많을경우 댓글 페이지가 넘어감. 댓글 50게 넘어가면 페이지 생기는듯\n",
    "    if reply_cnt != 0:\n",
    "        replies = soup.find('ul', {'class': 'fdb_lst_ul'})#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        for reply in replies:\n",
    "            reply_author = reply.find('span', {'class': 'name'}).get_text(strip=True)\n",
    "            reply_text = reply.find('span', {'class': 're_txt'}).get_text(strip=True).replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "            reply_content.append({'author': reply_author, 'text': reply_text})\n",
    "\n",
    "    data = {'title': title, 'user_id': user_id, 'post_time': post_time, 'post': post, 'view_cnt': view_cnt,\n",
    "            'recomm_cnt': recomm_cnt, 'reply_cnt': reply_cnt, 'reply_content': reply_content}\n",
    "\n",
    "    # 댓글이 있다면 딕셔너리에 추가\n",
    "    if reply_cnt != 0:\n",
    "        for i, reply_info in enumerate(reply_content):\n",
    "            data[f'reply_{i+1}_author'] = reply_info['author']\n",
    "            data[f'reply_{i+1}_text'] = reply_info['text']\n",
    "\n",
    "    print(url, '완료')\n",
    "    return data\n",
    "\n",
    "def get_contents(posts):\n",
    "    for post_link in posts:\n",
    "        content = extract_info(post_link)\n",
    "        append_to_file(f\"MLBPARK_{QUERY}_0228.csv\", content)\n",
    "    return print(\"모든 작업이 완료되었습니다.\")\n",
    "\n",
    "def save_to_file():\n",
    "    file = open(f'MLBPARK_{QUERY}_0228.csv', mode='a', encoding='utf-8')\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['title', 'user_id', 'post_time', 'post', 'view_cnt', 'recomm_cnt', 'reply_cnt', 'reply_author', 'reply_content'])\n",
    "    file.close()\n",
    "    return file\n",
    "\n",
    "def append_to_file(file_name, data):\n",
    "    file = open(file_name, mode='a', encoding='utf-8', newline='')  # newline 추가\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write main post data to CSV\n",
    "    main_post_row = [\n",
    "        data['title'],\n",
    "        data['user_id'],\n",
    "        data['post_time'],\n",
    "        data['post'],\n",
    "        data['view_cnt'],\n",
    "        data['recomm_cnt'],\n",
    "        data['reply_cnt']\n",
    "    ]\n",
    "\n",
    "    # Check if there are replies\n",
    "    if 'reply_content' in data and data['reply_cnt'] != 0:\n",
    "        # Write main post data along with reply data to CSV\n",
    "        for i, reply_info in enumerate(data['reply_content']):\n",
    "            writer.writerow(main_post_row + [\n",
    "                reply_info.get('author', ''),  # Reply author\n",
    "                reply_info.get('text', '')  # Reply text\n",
    "            ])\n",
    "    else:\n",
    "        # Write only main post data to CSV\n",
    "        writer.writerow(main_post_row + ['']*2)\n",
    "\n",
    "    file.close()\n",
    "    return\n",
    "\n",
    "# 크롤링할 페이지 수 설정\n",
    "PAGES = 50\n",
    "# 게시글 링크 수집\n",
    "post_links = get_posts(PAGES)\n",
    "# 결과를 저장할 CSV 파일 생성\n",
    "save_to_file()\n",
    "# 게시글 정보 크롤링 및 저장\n",
    "get_contents(post_links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bearstxt",
   "language": "python",
   "name": "bearstxt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
