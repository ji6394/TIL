## 경사하강법
1.
- 손실함수의 최솟값을 찾기 위한 알고리즘
- W(new) = W(old)-a(학습률)*dL/dW(old)
- 경사하강법 하이퍼파라미터
    - W(old)의 초기값
    - learning rate
    - 종료조건
- 경사하강법 함수 구현
``` python
#MyGrad 라이브러리 사용
def gradient_step(tensors, learning_rate):
    if isinstance(tensors, mg.Tensor):
        tensors=[tensors]

    for tensor in tensors:
        if tensor.grad is not None:
            tensor.data -= learning_rate*tensor.grad
```
