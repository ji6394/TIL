{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf41350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3bc4626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#시드 고정\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "221b2c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID 열은 필요 없으므로 로드와 함께 드랍\n",
    "train = pd.read_csv('./train.csv').drop('ID',axis=1) \n",
    "test = pd.read_csv('./test.csv').drop('ID',axis=1)\n",
    "submission = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad4a230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "cols = ['first_party','second_party','facts']\n",
    "shortword = re.compile(r'\\*\\b\\w{1}\\b') #비문자(x개)+띄어쓰기+문자하나+띄어쓰기\n",
    "tokenizer = TreebankWordTokenizer() #tokenization 패키지\n",
    "stopword = stopwords.words('english') #영어 stopword 로드\n",
    "lemmatizer = WordNetLemmatizer() #표제어추출 패키지\n",
    "# stemming(어간추출) : 문맥고려 X vs lemmatizer(표제어추출) : 문맥고려 O\n",
    "# Lemmatizer 은 정확한 분석을 위해 PoS(품사) 정보를 추가로 입력받음\n",
    "# 전처리 함수 1\n",
    "def preprocessing(df, cols, shortword, tokenizer, stopword, lemmatizer):\n",
    "    first_party_lst = []\n",
    "    second_party_lst=[]\n",
    "    facts_lst = []\n",
    "    for col in cols:\n",
    "        #좌우 공백 제거\n",
    "        df[col] = df[col].str.strip()\n",
    "        # 두칸 이상의 공백을 한칸으로 변경\n",
    "        df[col] = df[col].str.replace('  ', ' ')\n",
    "        # 소문자로 변경\n",
    "        df[col] = df[col].str.lower()\n",
    "        # \",\", \".\" 제거\n",
    "        df[col] = df[col].str.replace(',','')\n",
    "        df[col] = df[col].str.replace('.','') #.을 모두 제거함으로써 특정 의미 반영이 힘들 수 있을 듯\n",
    "        \n",
    "        if col=='first_party':#원고 column에 대해 다음 적용\n",
    "            for sample in df[col]:\n",
    "                # 한글자 단어 제거\n",
    "                sample = shortword.sub('', sample) #sample 중 한글자 단어를 ''으로 치환\n",
    "                #특수문자 제거\n",
    "                sample = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\",\"\",sample) #\\uAC00-\\uD7A30 : 모든 한글 음절, \\s : 띄어쓰기\n",
    "                #tokenizer를 이용한 단어 토큰화\n",
    "                token = tokenizer.tokenize(sample)\n",
    "                #불용어 제거\n",
    "                new_token=[]\n",
    "                for tok in token:\n",
    "                    if tok not in stopword:\n",
    "                        #표제어 추출(명사)\n",
    "                        new_token.append(lemmatizer.lemmatize(tok,'n'))\n",
    "                first_party_lst.append(new_token)\n",
    "                #sklearn.feature_extraction 변환을 위해 단어들을 결합?? : 문자를 숫자 벡터로 변환하기 위하여 BoW 인코딩 벡터를 만들어야 하는데 이에 사용되기 위해 각 first party들은 한 단어로 결합되어 있어야 함\n",
    "            for i in range(len(first_party_lst)):\n",
    "                first_party_lst[i] = ''.join(first_party_lst[i])\n",
    "        elif col == 'second_party': #피고 column에 대해 다음 적용\n",
    "            for sample in df[col]:\n",
    "                # 한글자 단어 제거\n",
    "                sample = shortword.sub('',sample)\n",
    "                #특수문자 제거\n",
    "                sample = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\",\"\",sample) #\\uAC00-\\uD7A30 : 모든 한글 음절, \\s : 띄어쓰기\n",
    "                #tokenizer를 이용한 단어 토큰화\n",
    "                token = tokenizer.tokenize(sample)\n",
    "                #불용어 제거\n",
    "                new_token=[]\n",
    "                for tok in token:\n",
    "                    if tok not in stopword:\n",
    "                        #표제어 추출(명사)\n",
    "                        new_token.append(lemmatizer.lemmatize(tok,'n'))\n",
    "                second_party_lst.append(new_token)\n",
    "                #sklearn.feature_extraction 변환을 위해 단어들을 결합?? : 문자를 숫자 벡터로 변환하기 위하여 BoW 인코딩 벡터를 만들어야 하는데 이에 사용되기 위해 각 first party들은 한 단어로 결합되어 있어야 함\n",
    "            for i in range(len(second_party_lst)):\n",
    "                second_party_lst[i] = ''.join(second_party_lst[i])\n",
    "        elif col=='facts': #왜 facts column에서는 표제어 추출을 하지 않았을까??*************************\n",
    "            for sample in df[col]:\n",
    "                # 한글자 단어 제거\n",
    "                sample = shortword.sub('', sample)\n",
    "                # 특수문자 제거\n",
    "                sample = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", sample)\n",
    "                # tokenzier를 이용한 단어 토큰화\n",
    "                token = tokenizer.tokenize(sample)\n",
    "                # 불용어 제거\n",
    "                new_token = []\n",
    "                for tok in token:\n",
    "                    if tok not in stopword:\n",
    "                        new_token.append(tok)\n",
    "                facts_lst.append(new_token)\n",
    "            # sklearn.feature_extraction 변환을 위해 단어들을 결합\n",
    "            for i in range(len(facts_lst)):\n",
    "                facts_lst[i] = ' '.join(facts_lst[i])\n",
    "        else:\n",
    "            print('컬럼이름을 변경하지 말아주세용')\n",
    "    return first_party_lst, second_party_lst, facts_lst\n",
    "\n",
    "#전처리함수 2(벡터화)\n",
    "def preprocessing_2(first, second, facts, vec, vec_facts, train=True):\n",
    "    if train:\n",
    "        vec.fit(first + second)\n",
    "        vec_facts.fit(facts)\n",
    "    \n",
    "    X1 = vec.transform(first).toarray()\n",
    "    X2 = vec.transform(second).toarray()\n",
    "    X3 = vec_facts.transform(facts).toarray()\n",
    "    \n",
    "    \n",
    "    return np.concatenate([X1,X2,X3], axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa6b1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_33548\\1584364167.py:23: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df[col] = df[col].str.replace('.','') #.을 모두 제거함으로써 특정 의미 반영이 힘들 수 있을 듯\n"
     ]
    }
   ],
   "source": [
    "# 문자열 전처리 1\n",
    "cols = ['first_party', 'second_party', 'facts']\n",
    "shortword = re.compile(r'\\W*\\b\\w{1}\\b')\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "stopword = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "first_train, second_train, facts_train = preprocessing(train, cols, shortword, tokenizer, stopword, lemmatizer)\n",
    "first_test, second_test, facts_test = preprocessing(test, cols, shortword, tokenizer, stopword, lemmatizer)\n",
    "\n",
    "# 문자열 전처리 2(벡터화)\n",
    "vec = CountVectorizer(ngram_range=(1,2)) #단어들의 출현 빈도로 문서 벡터화(모두 소문자로 변환) : 카운트 값이 높을 수록 중요한 단어로 인식\n",
    "vec_facts = TfidfVectorizer(ngram_range=(1,2)) # 개별 문서에 자주 등장하는 단어에 높은 가중치를 주되, 모든 문서에 전반적으로 자주 나오는 단어의 경우 페널티 부여\n",
    "\n",
    "X_train = preprocessing_2(first_train, second_train, facts_train, vec, vec_facts)\n",
    "y_train = train['first_party_winner']\n",
    "X_test = preprocessing_2(first_test, second_test, facts_test, vec, vec_facts, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e004b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 준비된 데이터 shape 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25058e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<train 데이터>\n",
      "(2478, 196474) (2478,)\n",
      "\n",
      "<test 데이터>\n",
      "(1240, 196474)\n"
     ]
    }
   ],
   "source": [
    "print('<train 데이터>')\n",
    "print(X_train.shape, y_train.shape)\n",
    "print()\n",
    "print('<test 데이터>')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "220c0223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1649\n",
      "0     829\n",
      "Name: first_party_winner, dtype: int64\n",
      "Train Data Shape after UnderSampling\n",
      "(1452, 196474) (1452,)\n",
      "====================\n",
      "Train target after UnderSampling\n",
      "0    829\n",
      "1    623\n",
      "Name: first_party_winner, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#언더샘플링 : Neighbourhood Cleaning rule( CondensedNearestNEighbour과 EditedNearesNeighbours를 섞은 방법)\n",
    "print(y_train.value_counts())\n",
    "X_nc, y_nc = NeighbourhoodCleaningRule(n_neighbors=3).fit_resample(X_train, y_train)\n",
    "print('Train Data Shape after UnderSampling')\n",
    "print(X_nc.shape, y_nc.shape)\n",
    "print('='*20)\n",
    "print('Train target after UnderSampling')\n",
    "print(y_nc.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9dd4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
