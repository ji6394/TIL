{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c65946f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "sub = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9cf733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United States       154\n",
       "Illinois              9\n",
       "Maryland              8\n",
       "Florida               8\n",
       "New York              7\n",
       "                   ... \n",
       "David Carpenter       1\n",
       "Larry Gene Heath      1\n",
       "PGA TOUR, Inc.        1\n",
       "PPL Montana, LLC      1\n",
       "Markman               1\n",
       "Name: first_party, Length: 2110, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['first_party'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "998517d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United States                        240\n",
       "California                            19\n",
       "United States of America              15\n",
       "Illinois                              13\n",
       "Federal Communications Commission     10\n",
       "                                    ... \n",
       "David Boren, Governor of Oklahoma      1\n",
       "Federal Bureau of Prisons et al.       1\n",
       "Town of Harrison                       1\n",
       "Charles Burr et al.                    1\n",
       "Westview Instruments, Inc.             1\n",
       "Name: second_party, Length: 1974, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['second_party'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30a0c595",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       On June 27, 1962, Phil St. Amant, a candidate ...\n",
       "1       Ramon Nelson was riding his bike when he suffe...\n",
       "2       An Alabama state court convicted Billy Joe Mag...\n",
       "3       Victor Linkletter was convicted in state court...\n",
       "4       On April 24, 1953 in Selma, Alabama, an intrud...\n",
       "                              ...                        \n",
       "2473    Congress amended the Clean Air Act through the...\n",
       "2474    Alliance Bond Fund, Inc., an investment fund, ...\n",
       "2475    In 1992, the District Court sentenced Manuel D...\n",
       "2476    On March 8, 1996, Enrico St. Cyr, a lawful per...\n",
       "2477    Herbert Markman owns the patent to a system th...\n",
       "Name: facts, Length: 2478, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['facts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2736ec5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    Ramon Nelson was riding his bike when he suffe...\n",
       "Name: facts, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['facts'][1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2c2b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(df):\n",
    "    tf1 = (df['facts'][1:2]).apply(lambda x:pd.value_counts(x.split(''))).sum(axis=0).reset_index()\n",
    "    tf1.columns=['words','tf']\n",
    "    return tf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2fa0af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       On June 27, 1962, Phil St. Amant, a candidate ...\n",
       "1       Ramon Nelson was riding his bike when he suffe...\n",
       "2       An Alabama state court convicted Billy Joe Mag...\n",
       "3       Victor Linkletter was convicted in state court...\n",
       "4       On April 24, 1953 in Selma, Alabama, an intrud...\n",
       "                              ...                        \n",
       "2473    Congress amended the Clean Air Act through the...\n",
       "2474    Alliance Bond Fund, Inc., an investment fund, ...\n",
       "2475    In 1992, the District Court sentenced Manuel D...\n",
       "2476    On March 8, 1996, Enrico St. Cyr, a lawful per...\n",
       "2477    Herbert Markman owns the patent to a system th...\n",
       "Name: facts, Length: 2478, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['facts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c88e64f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>was</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Owens’</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>distribution,</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>potentially</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>crack</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>trial.\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             index   0\n",
       "0              the  12\n",
       "1                a   8\n",
       "2              was   8\n",
       "3              and   8\n",
       "4               of   7\n",
       "..             ...  ..\n",
       "117         Owens’   1\n",
       "118  distribution,   1\n",
       "119    potentially   1\n",
       "120          crack   1\n",
       "121       trial.\\n   1\n",
       "\n",
       "[122 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['facts'][1:2].apply(lambda x: pd.value_counts(x.split(' '))).sum(axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62c80349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       On June 27, 1962, Phil St. Amant, a candidate ...\n",
       "1       Ramon Nelson was riding his bike when he suffe...\n",
       "2       An Alabama state court convicted Billy Joe Mag...\n",
       "3       Victor Linkletter was convicted in state court...\n",
       "4       On April 24, 1953 in Selma, Alabama, an intrud...\n",
       "                              ...                        \n",
       "2473    Congress amended the Clean Air Act through the...\n",
       "2474    Alliance Bond Fund, Inc., an investment fund, ...\n",
       "2475    In 1992, the District Court sentenced Manuel D...\n",
       "2476    On March 8, 1996, Enrico St. Cyr, a lawful per...\n",
       "2477    Herbert Markman owns the patent to a system th...\n",
       "Name: facts, Length: 2478, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 출력 옵션 설정\n",
    "pd.set_option('display.max_columns',None) #모든 열 표시\n",
    "pd.set_option('display.width', None) # 줄 바꿈 없이 모든 내용 표시\n",
    "data['facts']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4985e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ramon Nelson was riding his bike when he suffered a lethal blow to the back of his head with a baseball bat. After two eyewitnesses identified Lawrence Owens from an array of photos and then a lineup, he was tried and convicted for Nelson’s death. Because Nelson was carrying cocaine and crack cocaine potentially for distribution, the judge at Owens’ bench trial ruled that Owens was probably also a drug dealer and was trying to “knock [Nelson] off.” Owens was found guilty of first-degree murder and sentenced to 25 years in prison.\\nOwens filed a petition for a writ of habeas corpus on the grounds that his constitutional right to due process was violated during the trial. He argued that the eyewitness identification should have been inadmissible based on unreliability and that the judge impermissibly inferred a motive when a motive was not an element of the offense. The district court denied the writ of habeas corpus, and Owens appealed. The U.S. Court of Appeals for the Seventh Circuit reversed the denial and held that the trial judge’s inference about Owens’s motive violated his right to have his guilt adjudicated solely based on the evidence presented at trial.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[1]['facts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5e3a966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On June 27, 1962, Phil St. Amant, a candidate for public office, made a television speech in Baton Rouge, Louisiana.  During this speech, St. Amant accused his political opponent of being a Communist and of being involved in criminal activities with the head of the local Teamsters Union.  Finally, St. Amant implicated Herman Thompson, an East Baton Rouge deputy sheriff, in a scheme to move money between the Teamsters Union and St. Amant’s political opponent. \\nThompson successfully sued St. Amant for defamation.  Louisiana’s First Circuit Court of Appeals reversed, holding that Thompson did not show St. Amant acted with “malice.”  Thompson then appealed to the Supreme Court of Louisiana.  That court held that, although public figures forfeit some of their First Amendment protection from defamation, St. Amant accused Thompson of a crime with utter disregard of whether the remarks were true.  Finally, that court held that the First Amendment protects uninhibited, robust debate, rather than an open season to shoot down the good name of anyone who happens to be a public servant. \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0]['facts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa65510a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1649\n",
       "0     829\n",
       "Name: first_party_winner, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['first_party_winner'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81ba7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#다운샘플링\n",
    "from sklearn.utils import resample\n",
    "\n",
    "subset_0 = data[data['first_party_winner']==0]\n",
    "subset_1 = data[data['first_party_winner']==1]\n",
    "\n",
    "subset_1_downsampled = resample(subset_1, replace=False, n_samples=800, random_state=42)\n",
    "\n",
    "data = pd.concat([subset_0, subset_1_downsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ea5abf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    829\n",
       "1    800\n",
       "Name: first_party_winner, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['first_party_winner'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6991d5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\82108\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\82108\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\82108\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers.tokenization_utils import TruncationStrategy\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def stem_text_func(text):\n",
    "    token_words = word_tokenize(text)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(' ')\n",
    "    return ''.join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7db8e710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stop_words in c:\\users\\82108\\anaconda3\\envs\\new\\lib\\site-packages (2018.7.23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\82108\\anaconda3\\envs\\new\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\82108\\anaconda3\\envs\\new\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\82108\\anaconda3\\envs\\new\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\82108\\anaconda3\\envs\\new\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\82108\\anaconda3\\envs\\new\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\82108\\anaconda3\\envs\\new\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25236e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "#불용어 리스트\n",
    "languages=['English']\n",
    "my_stop_word_list=[]\n",
    "for i in languages:\n",
    "    my_stop_word_list.append(get_stop_words(i.lower()))\n",
    "my_stop_word_list = sum(my_stop_word_list, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2996c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " \"here's\",\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " \"how's\",\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"let's\",\n",
       " 'me',\n",
       " 'more',\n",
       " 'most',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'same',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that's\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " \"there's\",\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'very',\n",
       " 'was',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " \"what's\",\n",
       " 'when',\n",
       " \"when's\",\n",
       " 'where',\n",
       " \"where's\",\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " \"who's\",\n",
       " 'whom',\n",
       " 'why',\n",
       " \"why's\",\n",
       " 'with',\n",
       " \"won't\",\n",
       " 'would',\n",
       " \"wouldn't\",\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_stop_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54f12bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/nealrs/96342d8231b75cf4bb82\n",
    "contractions = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",\n",
    "  \"you'll've\": \"you you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17d3c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#main 단어만 추출 -> 차후 이 단어들로 test\n",
    "def preprocessing_sentence(sentence, remove_stopwords=True):\n",
    "    #소문자화\n",
    "    sentence = sentence.lower()\n",
    "    # /n 제거\n",
    "    sentence = re.sub(r'/n','',sentence)\n",
    "    # 괄호로 닫힌 문자열 괄호 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)','',sentence)\n",
    "    # 쌍따옴표 제거\n",
    "    sentence = re.sub('\"','',sentence)\n",
    "    # 약어 정규화\n",
    "    sentence = ''.join([contractions[t] if t in contractions else t for t in sentence.split(' ')])\n",
    "    # 소유격 제거\n",
    "    sentence = re.sub(r\"'s\\b\",\" \", sentence)\n",
    "    #특수문자 제거\n",
    "    sentence = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "    \n",
    "    # 불용어 제거\n",
    "    if my_stop_word_list:\n",
    "        tokens = ''.join(word for word in sentence.split() if not word in my_stop_word_list if len(word)>1)\n",
    "    else:\n",
    "        tokens = ''.join(word for word in sentence.split() if len(word)>1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "489f4d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['facts'] = [preprocessing_sentence(i) for i in data['facts']]\n",
    "test['facts'] = [preprocessing_sentence(i) for i in test['facts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48abf3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['facts'] = data['facts'].astype('str')\n",
    "test['facts'] = test['facts'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b63c7bd",
   "metadata": {},
   "source": [
    "# Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03e04dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "def get_vector(vectorizer, df, train_mode):\n",
    "    if train_mode:\n",
    "        X_facts = vectorizer.fit_transform(df['facts'])\n",
    "    else:\n",
    "        X_facts = vectorizer.transform(df['facts'])\n",
    "    X_party1 = vectorizer.transform(df['first_party'])\n",
    "    X_party2 = vectorizer.transform(df['second_party'])\n",
    "    \n",
    "    X = np.concatenate([X_party1.todense(), X_party2.todense(), X_facts.todense()], axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba2d155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_vector(vectorizer, data, True)\n",
    "Y = data['first_party_winner']\n",
    "X_t = get_vector(vectorizer, test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2645cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.squeeze(np.asarray(X))\n",
    "X_T = np.squeeze(np.asarray(X_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "afd0ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "X_T_scaled = scaler.transform(X_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4754c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm \n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ca4dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e38c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_params = {\n",
    "    'verbose' : 0,\n",
    "    'random_state' : 113,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c218a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# Classifiers\n",
    "names = [\n",
    "    \"Logistic Regression\",\n",
    "    \"KNN Classifier\",\n",
    "    \"Decision Tree\",\n",
    "    \"LinearSVC\",\n",
    "    \"Linear SVM\",\n",
    "    \"Poly SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"sigmoid SVM\",\n",
    "    \"Neural Network\",\n",
    "    \"Random Forest\",\n",
    "    \"SGD Classifier\",\n",
    "    \"Ridge Classifier\",\n",
    "    \"XGBoost\",\n",
    "    \"AdaBoost\",\n",
    "    \"Catboost\",\n",
    "    \"Gaussian Naive Bayes\"\n",
    "]\n",
    "\n",
    "models = [\n",
    "    LogisticRegression(max_iter = 1000),\n",
    "    KNeighborsClassifier(n_neighbors = 149, n_jobs = -1),\n",
    "    DecisionTreeClassifier(),\n",
    "    LinearSVC(random_state=113),\n",
    "    svm.SVC(kernel = 'linear'),\n",
    "    svm.SVC(kernel = 'poly'),\n",
    "    svm.SVC(kernel = 'rbf'),\n",
    "    svm.SVC(kernel = 'sigmoid'),\n",
    "    MLPClassifier(),\n",
    "    RandomForestClassifier(n_estimators = 100),\n",
    "    SGDClassifier(loss = 'hinge'),\n",
    "    RidgeClassifier(),\n",
    "    XGBClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    CatBoostClassifier(**cat_params),\n",
    "    GaussianNB()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "629e6a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Function to return summary of baseline models\n",
    "def score(X_train, y_train, X_val, y_val, names = names, models = models):\n",
    "    score_df, score_train, score_val = pd.DataFrame(), [], []\n",
    "    x = time.time()\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_train_pred, y_val_pred = model.predict(X_train), model.predict(X_val)\n",
    "        score_train.append(accuracy_score(y_train, y_train_pred))\n",
    "        score_val.append(accuracy_score(y_val, y_val_pred))\n",
    "    \n",
    "    score_df[\"Classifier\"], score_df[\"Training accuracy\"], score_df[\"Validation accuracy\"] = names, score_train, score_val\n",
    "    score_df.sort_values(by = 'Validation accuracy', ascending = False, inplace = True)\n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf669318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Training accuracy</th>\n",
       "      <th>Validation accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.57362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.42638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN Classifier</td>\n",
       "      <td>0.507291</td>\n",
       "      <td>0.42638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.42638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.42638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.42638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Poly SVM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.42638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RBF SVM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.42638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sigmoid SVM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.42638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.42638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.42638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.42638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.42638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.507291</td>\n",
       "      <td>0.42638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.545664</td>\n",
       "      <td>0.42638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.884114</td>\n",
       "      <td>0.42638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Classifier  Training accuracy  Validation accuracy\n",
       "15  Gaussian Naive Bayes           1.000000              0.57362\n",
       "0    Logistic Regression           1.000000              0.42638\n",
       "1         KNN Classifier           0.507291              0.42638\n",
       "2          Decision Tree           1.000000              0.42638\n",
       "3              LinearSVC           1.000000              0.42638\n",
       "4             Linear SVM           1.000000              0.42638\n",
       "5               Poly SVM           1.000000              0.42638\n",
       "6                RBF SVM           1.000000              0.42638\n",
       "7            sigmoid SVM           1.000000              0.42638\n",
       "8         Neural Network           1.000000              0.42638\n",
       "9          Random Forest           1.000000              0.42638\n",
       "10        SGD Classifier           1.000000              0.42638\n",
       "11      Ridge Classifier           1.000000              0.42638\n",
       "12               XGBoost           0.507291              0.42638\n",
       "13              AdaBoost           0.545664              0.42638\n",
       "14              Catboost           0.884114              0.42638"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(X_train, Y_train, X_test, Y_test, names = names, models = models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c34ff2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy score: 1.0\n",
      "Testing Accuracy score: 0.4263803680981595\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=113)\n",
    "clf.fit(X_train, Y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "print('Training Accuracy score:', accuracy_score(Y_train, y_pred_train))\n",
    "print('Testing Accuracy score:', accuracy_score(Y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48433149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
